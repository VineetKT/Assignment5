{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGQmYwI738B_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "wquBfVH54FpC",
    "outputId": "485e25f1-3103-4fd9-85b4-3d71bdd38869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c3f204f6e039>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/vinitkumar/Documents/Quizzes&Assgn/Assgn5/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/vinitkumar/Documents/Quizzes&Assgn/Assgn5/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/vinitkumar/Documents/Quizzes&Assgn/Assgn5/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/vinitkumar/Documents/Quizzes&Assgn/Assgn5/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('/home/vinitkumar/Documents/Quizzes&Assgn/Assgn5', one_hot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PhL4QoWH4OAP",
    "outputId": "8dc8c0fe-1605-4366-b1dd-485323440a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-set:  55000\n",
      "Test-set:  10000\n",
      "Validation-set:  5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training-set: \", len(data.train.labels))\n",
    "print(\"Test-set: \", len(data.test.labels))\n",
    "print(\"Validation-set: \", len(data.validation.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOspNNNb4Oz7"
   },
   "outputs": [],
   "source": [
    "# Placeholder for the input images\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28*28], name='X')\n",
    "# Reshape it into [num_images, img_height, img_width, num_channels]\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# Placeholder for the true labels of images\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfAmsw3i4O-3"
   },
   "outputs": [],
   "source": [
    "#Function for creating a new Convolution Layer\n",
    "def Convolution(input_, in_channels, filter_size, out_channels, name):    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        shape_ = [filter_size, filter_size, in_channels, out_channels]    # Shape of the filter-weights for the convolution\n",
    "        filters = tf.Variable(tf.truncated_normal(shape= shape_, stddev= 0.05))\n",
    "\n",
    "        biases = tf.Variable(tf.constant(0.05, shape= [out_channels]))   # one bias for each filter\n",
    "\n",
    "        layer = tf.nn.conv2d(input= input_, filter= filters, strides= [1, 1, 1, 1], padding= 'SAME', data_format='NHWC')\n",
    "        layer += biases # adding the bias term after filtering\n",
    "        return layer, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOQYCif44PKF"
   },
   "outputs": [],
   "source": [
    "#Function for creating a new Pooling Layer\n",
    "def pooling(input_, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        layer = tf.nn.max_pool(value= input_, ksize= [1, 2, 2, 1], strides= [1, 1, 1, 1], padding= 'SAME', data_format='NHWC')\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpYWaDEZ4PQf"
   },
   "outputs": [],
   "source": [
    "#Function for creating a new ReLU Layer\n",
    "def Relu(input_, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        layer = tf.nn.relu(input_)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUamgNXJ4PTQ"
   },
   "outputs": [],
   "source": [
    "#Function for creating a new Fully connected Layer\n",
    "def FC(input_, num_inputs, num_outputs, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape= [num_inputs, num_outputs], stddev= 0.05))\n",
    "        biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "        \n",
    "        layer = tf.matmul(input_, weights) + biases\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRv6wcdX4PV4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitkumar/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Create Convolutional Neural Network\n",
    "#-------------Convolutional Layer 1-------------------#\n",
    "l1_conv, l1_filters = Convolution(input_= x_image, in_channels= 1, filter_size= 5, out_channels= 6, name = \"conv1\")\n",
    "l1_pool = pooling(l1_conv, name=\"pool1\")\n",
    "l1_relu = Relu(l1_pool, name=\"relu1\")\n",
    "\n",
    "#--------------Convolutional Layer 2-----------------#\n",
    "l2_conv, l2_filters = Convolution(input_= l1_relu, in_channels= 6, filter_size= 5, out_channels= 16, name= \"conv2\")\n",
    "l2_pool = pooling(l2_conv, name=\"pool2\")\n",
    "l2_relu = Relu(l2_pool, name=\"relu2\")\n",
    "\n",
    "# Flattening the layer\n",
    "num_features = l2_relu.get_shape()[1:4].num_elements()\n",
    "flat = tf.reshape(l2_relu, [-1, num_features])\n",
    "\n",
    "# Fully-Connected Layer 1\n",
    "fc1 = FC(flat, num_inputs= num_features, num_outputs= 128, name= \"fc1\")\n",
    "\n",
    "# RelU layer 3\n",
    "fc1_relu = Relu(fc1, name= \"relu3\")\n",
    "\n",
    "# Fully-Connected Layer 2\n",
    "fc2 = FC(fc1_relu, num_inputs= 128, num_outputs= 10, name= \"fc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1q6iQW54PYP"
   },
   "outputs": [],
   "source": [
    "# Softmax function to normalize the output\n",
    "with tf.variable_scope(\"Softmax\"):\n",
    "    y_pred = tf.nn.softmax(fc2)\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGY06MCa4Pb7"
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost function\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= fc2, labels= y_true)\n",
    "    cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "6oWL0Nxl4PeV",
    "outputId": "877afe03-7e9d-4e6e-ff23-da31e67088a0"
   },
   "outputs": [],
   "source": [
    "# Adam Optimizer\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= 1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43cpey-f4PmJ"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW-jVeKt4_dz"
   },
   "outputs": [],
   "source": [
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(\"TrainingGraph/\")\n",
    "writer1 = tf.summary.FileWriter(\"ValidatGraph/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRkApyzI4_gg"
   },
   "outputs": [],
   "source": [
    "# Add the cost and accuracy to summary\n",
    "tf.summary.scalar('loss', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp8ms_Dp4_ib"
   },
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "id": "qgKjkNMc4_l0",
    "outputId": "b06da7d7-fe6b-4f98-c59b-93f68932994a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training Accuracy:  49.470909183675595\n",
      "Validation Accuracy:  71.11999988555908\n",
      "Iteration:  2\n",
      "Training Accuracy:  72.27818174795671\n",
      "Validation Accuracy:  74.6999979019165\n",
      "Iteration:  3\n",
      "Training Accuracy:  75.5181820826097\n",
      "Validation Accuracy:  78.17999720573425\n",
      "Iteration:  4\n",
      "Training Accuracy:  78.32545453851874\n",
      "Validation Accuracy:  80.55999875068665\n",
      "Iteration:  5\n",
      "Training Accuracy:  80.40181821042842\n",
      "Validation Accuracy:  81.94000124931335\n",
      "Iteration:  6\n",
      "Training Accuracy:  81.51090903715654\n",
      "Validation Accuracy:  83.09999704360962\n",
      "Iteration:  7\n",
      "Training Accuracy:  82.65272758223794\n",
      "Validation Accuracy:  83.63999724388123\n",
      "Iteration:  8\n",
      "Training Accuracy:  83.58545455065641\n",
      "Validation Accuracy:  83.93999934196472\n",
      "Iteration:  9\n",
      "Training Accuracy:  84.07999981533398\n",
      "Validation Accuracy:  84.92000102996826\n",
      "Iteration:  10\n",
      "Training Accuracy:  84.58727283911271\n",
      "Validation Accuracy:  85.47999858856201\n",
      "Iteration:  11\n",
      "Training Accuracy:  84.98909115791321\n",
      "Validation Accuracy:  85.93999743461609\n",
      "Iteration:  12\n",
      "Training Accuracy:  85.43999986215071\n",
      "Validation Accuracy:  85.79999804496765\n",
      "Iteration:  13\n",
      "Training Accuracy:  85.69272778250955\n",
      "Validation Accuracy:  85.65999865531921\n",
      "Iteration:  14\n",
      "Training Accuracy:  85.80727273767644\n",
      "Validation Accuracy:  86.69999837875366\n",
      "Iteration:  15\n",
      "Training Accuracy:  86.36181766336615\n",
      "Validation Accuracy:  86.87999844551086\n",
      "Iteration:  16\n",
      "Training Accuracy:  86.42363645813683\n",
      "Validation Accuracy:  86.9599997997284\n",
      "Iteration:  17\n",
      "Training Accuracy:  86.61999962546608\n",
      "Validation Accuracy:  87.1399998664856\n",
      "Iteration:  18\n",
      "Training Accuracy:  86.77818157456137\n",
      "Validation Accuracy:  87.26000189781189\n",
      "Iteration:  19\n",
      "Training Accuracy:  87.05636371265759\n",
      "Validation Accuracy:  87.55999803543091\n",
      "Iteration:  20\n",
      "Training Accuracy:  87.08909099752252\n",
      "Validation Accuracy:  87.36000061035156\n",
      "Iteration:  21\n",
      "Training Accuracy:  87.267273014242\n",
      "Validation Accuracy:  87.81999945640564\n",
      "Iteration:  22\n",
      "Training Accuracy:  87.59818207133901\n",
      "Validation Accuracy:  87.8000020980835\n",
      "Iteration:  23\n",
      "Training Accuracy:  87.62545466423035\n",
      "Validation Accuracy:  87.6200020313263\n",
      "Iteration:  24\n",
      "Training Accuracy:  87.809090831063\n",
      "Validation Accuracy:  88.20000290870667\n",
      "Iteration:  25\n",
      "Training Accuracy:  87.91090911084956\n",
      "Validation Accuracy:  88.08000087738037\n",
      "Iteration:  26\n",
      "Training Accuracy:  88.02909070795234\n",
      "Validation Accuracy:  87.91999816894531\n",
      "Iteration:  27\n",
      "Training Accuracy:  88.01454544067383\n",
      "Validation Accuracy:  88.12000155448914\n",
      "Iteration:  28\n",
      "Training Accuracy:  88.13636367971247\n",
      "Validation Accuracy:  88.40000033378601\n",
      "Iteration:  29\n",
      "Training Accuracy:  88.20363640785217\n",
      "Validation Accuracy:  88.13999891281128\n",
      "Iteration:  30\n",
      "Training Accuracy:  88.36909088221464\n",
      "Validation Accuracy:  88.58000040054321\n",
      "Iteration:  31\n",
      "Training Accuracy:  88.59454545107755\n",
      "Validation Accuracy:  88.22000026702881\n",
      "Iteration:  32\n",
      "Training Accuracy:  88.48545464602384\n",
      "Validation Accuracy:  88.58000040054321\n",
      "Iteration:  33\n",
      "Training Accuracy:  88.60181873494929\n",
      "Validation Accuracy:  88.63999843597412\n",
      "Iteration:  34\n",
      "Training Accuracy:  88.71636369011618\n",
      "Validation Accuracy:  88.22000026702881\n",
      "Iteration:  35\n",
      "Training Accuracy:  88.80909074436535\n",
      "Validation Accuracy:  88.62000107765198\n",
      "Iteration:  36\n",
      "Training Accuracy:  88.85454502972689\n",
      "Validation Accuracy:  88.7000024318695\n",
      "Iteration:  37\n",
      "Training Accuracy:  88.90545465729454\n",
      "Validation Accuracy:  88.94000053405762\n",
      "Iteration:  38\n",
      "Training Accuracy:  89.06181823123586\n",
      "Validation Accuracy:  88.89999985694885\n",
      "Iteration:  39\n",
      "Training Accuracy:  89.12000027569857\n",
      "Validation Accuracy:  88.66000175476074\n",
      "Iteration:  40\n",
      "Training Accuracy:  89.14545460180803\n",
      "Validation Accuracy:  89.12000060081482\n",
      "Iteration:  41\n",
      "Training Accuracy:  89.31818160143766\n",
      "Validation Accuracy:  89.06000256538391\n",
      "Iteration:  42\n",
      "Training Accuracy:  89.31636409326033\n",
      "Validation Accuracy:  88.81999850273132\n",
      "Iteration:  43\n",
      "Training Accuracy:  89.39090891317888\n",
      "Validation Accuracy:  89.21999931335449\n",
      "Iteration:  44\n",
      "Training Accuracy:  89.34909094463696\n",
      "Validation Accuracy:  89.21999931335449\n",
      "Iteration:  45\n",
      "Training Accuracy:  89.57454562187195\n",
      "Validation Accuracy:  89.12000060081482\n",
      "Iteration:  46\n",
      "Training Accuracy:  89.59090904756026\n",
      "Validation Accuracy:  89.12000060081482\n",
      "Iteration:  47\n",
      "Training Accuracy:  89.57818204706366\n",
      "Validation Accuracy:  89.2799973487854\n",
      "Iteration:  48\n",
      "Training Accuracy:  89.64909087527883\n",
      "Validation Accuracy:  88.77999782562256\n",
      "Iteration:  49\n",
      "Training Accuracy:  89.67454520138827\n",
      "Validation Accuracy:  89.25999999046326\n",
      "Iteration:  50\n",
      "Training Accuracy:  89.77272748947144\n",
      "Validation Accuracy:  89.34000134468079\n",
      "Iteration:  51\n",
      "Training Accuracy:  89.88545471971685\n",
      "Validation Accuracy:  89.17999863624573\n",
      "Iteration:  52\n",
      "Training Accuracy:  89.9454542723569\n",
      "Validation Accuracy:  88.89999985694885\n",
      "Iteration:  53\n",
      "Training Accuracy:  89.9381819638339\n",
      "Validation Accuracy:  89.06000256538391\n",
      "Iteration:  54\n",
      "Training Accuracy:  90.0309093431993\n",
      "Validation Accuracy:  89.2799973487854\n",
      "Iteration:  55\n",
      "Training Accuracy:  90.07454557852311\n",
      "Validation Accuracy:  89.34000134468079\n",
      "Iteration:  56\n",
      "Training Accuracy:  90.063636302948\n",
      "Validation Accuracy:  89.4599974155426\n",
      "Iteration:  57\n",
      "Training Accuracy:  90.16727295788851\n",
      "Validation Accuracy:  89.52000141143799\n",
      "Iteration:  58\n",
      "Training Accuracy:  90.30545473098755\n",
      "Validation Accuracy:  89.2799973487854\n",
      "Iteration:  59\n",
      "Training Accuracy:  90.4036359353499\n",
      "Validation Accuracy:  89.31999802589417\n",
      "Iteration:  60\n",
      "Training Accuracy:  90.36727276715365\n",
      "Validation Accuracy:  89.48000073432922\n",
      "Iteration:  61\n",
      "Training Accuracy:  90.49636363983154\n",
      "Validation Accuracy:  89.6399974822998\n",
      "Iteration:  62\n",
      "Training Accuracy:  90.5418184670535\n",
      "Validation Accuracy:  89.92000222206116\n",
      "Iteration:  63\n",
      "Training Accuracy:  90.48545490611684\n",
      "Validation Accuracy:  89.49999809265137\n",
      "Iteration:  64\n",
      "Training Accuracy:  90.56727268479086\n",
      "Validation Accuracy:  89.99999761581421\n",
      "Iteration:  65\n",
      "Training Accuracy:  90.66181833093817\n",
      "Validation Accuracy:  89.52000141143799\n",
      "Iteration:  66\n",
      "Training Accuracy:  90.63454562967473\n",
      "Validation Accuracy:  89.96000289916992\n",
      "Iteration:  67\n",
      "Training Accuracy:  90.65818190574646\n",
      "Validation Accuracy:  89.70000147819519\n",
      "Iteration:  68\n",
      "Training Accuracy:  90.79999977892096\n",
      "Validation Accuracy:  89.38000202178955\n",
      "Iteration:  69\n",
      "Training Accuracy:  90.6672728061676\n",
      "Validation Accuracy:  89.60000276565552\n",
      "Iteration:  70\n",
      "Training Accuracy:  90.7036362994801\n",
      "Validation Accuracy:  89.6399974822998\n",
      "Iteration:  71\n",
      "Training Accuracy:  90.87818156589161\n",
      "Validation Accuracy:  89.60000276565552\n",
      "Iteration:  72\n",
      "Training Accuracy:  90.99090890450911\n",
      "Validation Accuracy:  89.80000019073486\n",
      "Iteration:  73\n",
      "Training Accuracy:  90.96363620324568\n",
      "Validation Accuracy:  90.10000228881836\n",
      "Iteration:  74\n",
      "Training Accuracy:  91.01818193088879\n",
      "Validation Accuracy:  89.92000222206116\n",
      "Iteration:  75\n",
      "Training Accuracy:  91.10909115184437\n",
      "Validation Accuracy:  89.89999890327454\n",
      "Iteration:  76\n",
      "Training Accuracy:  91.14727247845042\n",
      "Validation Accuracy:  90.20000100135803\n",
      "Iteration:  77\n",
      "Training Accuracy:  91.19454578919844\n",
      "Validation Accuracy:  90.07999897003174\n",
      "Iteration:  78\n",
      "Training Accuracy:  91.18727283044295\n",
      "Validation Accuracy:  90.17999768257141\n",
      "Iteration:  79\n",
      "Training Accuracy:  91.3345456123352\n",
      "Validation Accuracy:  90.1199996471405\n",
      "Iteration:  80\n",
      "Training Accuracy:  91.29090883515097\n",
      "Validation Accuracy:  90.10000228881836\n",
      "Iteration:  81\n",
      "Training Accuracy:  91.29090905189514\n",
      "Validation Accuracy:  89.7599995136261\n",
      "Iteration:  82\n",
      "Training Accuracy:  91.34181813760237\n",
      "Validation Accuracy:  90.16000032424927\n",
      "Iteration:  83\n",
      "Training Accuracy:  91.37272715568542\n",
      "Validation Accuracy:  90.17999768257141\n",
      "Iteration:  84\n",
      "Training Accuracy:  91.37090943076393\n",
      "Validation Accuracy:  90.2999997138977\n",
      "Iteration:  85\n",
      "Training Accuracy:  91.56909097324719\n",
      "Validation Accuracy:  90.2999997138977\n",
      "Iteration:  86\n",
      "Training Accuracy:  91.43636345863342\n",
      "Validation Accuracy:  90.14000296592712\n",
      "Iteration:  87\n",
      "Training Accuracy:  91.46909041838212\n",
      "Validation Accuracy:  90.14000296592712\n",
      "Iteration:  88\n",
      "Training Accuracy:  91.62727290933782\n",
      "Validation Accuracy:  90.17999768257141\n",
      "Iteration:  89\n",
      "Training Accuracy:  91.57090869816867\n",
      "Validation Accuracy:  90.10000228881836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  90\n",
      "Training Accuracy:  91.74545472318476\n",
      "Validation Accuracy:  90.21999835968018\n",
      "Iteration:  91\n",
      "Training Accuracy:  91.7036364295266\n",
      "Validation Accuracy:  89.74000215530396\n",
      "Iteration:  92\n",
      "Training Accuracy:  91.63454532623291\n",
      "Validation Accuracy:  90.35999774932861\n",
      "Iteration:  93\n",
      "Training Accuracy:  91.71272722157565\n",
      "Validation Accuracy:  90.52000045776367\n",
      "Iteration:  94\n",
      "Training Accuracy:  91.76000020720741\n",
      "Validation Accuracy:  90.420001745224\n",
      "Iteration:  95\n",
      "Training Accuracy:  91.8218183517456\n",
      "Validation Accuracy:  90.53999781608582\n",
      "Iteration:  96\n",
      "Training Accuracy:  91.86000000346792\n",
      "Validation Accuracy:  90.38000106811523\n",
      "Iteration:  97\n",
      "Training Accuracy:  91.894545880231\n",
      "Validation Accuracy:  90.56000113487244\n",
      "Iteration:  98\n",
      "Training Accuracy:  91.93818200718272\n",
      "Validation Accuracy:  90.7800018787384\n",
      "Iteration:  99\n",
      "Training Accuracy:  92.03272743658586\n",
      "Validation Accuracy:  90.420001745224\n",
      "Iteration:  100\n",
      "Training Accuracy:  91.99636394327338\n",
      "Validation Accuracy:  90.47999978065491\n"
     ]
    }
   ],
   "source": [
    "#---------------------------TensorFlow Session------------------------#\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # Initializing the variables\n",
    "    \n",
    "    # Adding model graph to the tensorboard\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for iter_ in range(n_iter):\n",
    "        train_accuracy = 0\n",
    "        for batch in range(0, int(len(data.train.labels)/batch_size)):\n",
    "            # Getting a batch of images and labels\n",
    "            x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "            \n",
    "            # assigning the batch data into placeholder variables\n",
    "            feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "            \n",
    "            # optimizer for current batch\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # accuracy on the batch of training data\n",
    "            train_accuracy += sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # summary for current batch\n",
    "            summ = sess.run(merged_summary, feed_dict=feed_dict_train)\n",
    "            writer.add_summary(summ, iter_*int(len(data.train.labels)/batch_size) + batch)\n",
    "        \n",
    "        train_accuracy /= int(len(data.train.labels)/batch_size)\n",
    "        \n",
    "        # Generating summary and validating the model on the entire validation set\n",
    "        summ, valid_accuracy = sess.run([merged_summary, accuracy], feed_dict={x:data.validation.images, y_true:data.validation.labels})\n",
    "        writer1.add_summary(summ, iter_)\n",
    "          \n",
    "        print('Iteration: ', iter_+1)\n",
    "        print('Training Accuracy: ', train_accuracy*100)\n",
    "        print('Validation Accuracy: ', valid_accuracy*100)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x:data.test.images, y_true:data.test.labels})\n",
    "    print('Test Accuracy: ', test_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE3jCqdh508N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment5_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
